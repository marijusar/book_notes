For very large datasets or very large throughput replication is not sufficient. We need to break data into *partitions*, also known as *sharding*.

Normally, partitions are defined is such a way that each piece of data belongs to exactly one partition. Each partition is a small database of its own, even though database may support operations that support multiple partitions.

The main reason for wanting data partition is *scalability*. A large dataset can be distributed across many disks, and the query load can be distributed across many processors.

Partitioning is usually combined with replication, so that copies of each partition are stored on multiple nodes. This means that, even though each records belongs to exactly one partition, it may still be stored on several different nodes for fault tolerance. One node can be leader of one partition and follower to another partition. (DDIA page 201).

### Partitioning of key-value data

Our goal is to spread the data and the query load evenly across nodes. If every node takes a fair share, then in theory, 10 nodes should be able to handle 10 times as much data and 10 times the read and write throughput of a single node.

If the partitioning is unfair, so that some partitions have more data or queries than others, we call it *skewed*. A partition with disproportionately high load is called a *hot spot*.

One way of partitioning is to assign a continuous range of keys to each partition. If you know the boundaries between the range, you easily tell which partition contains a given key. The ranges of keys are not necessarily evenly spaces, because your data may not be evenly distributed. The partition boundaries might be chosen manually by an administrator, or database can choose them automatically.

Within each partition, we can keep keys in sorted order. This has the advantage that range scans are easy and you can treat the key as concatenated index order to fetch several related records in one query.

However, downside to key-range partitioning is that certain access patterns can lead to hot spots. E.g. if we have system where send information from some sensors and partition them by timestamp, then all of the writes for that day would go to one partition.

Because of this risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key. A good hash function takes skewed data and makes it uniformly distributed. Once you have a suitable hash function, you can assign each partition a range of hashes. This technique is good at distributing keys fairly among the partitions. The partition boundaries can be evenly spaced, or they can be chosen pseudo-randomly.

Unfortunately, by using the hash of the key for partitioning we lose a nice property of key-range partitioning : the ability to do efficient range queries. Keys that were once adjacent are now scattered across all partitions, so their sort order is lost.

Even when using hash keys, some partitions can get very hot. Imagine a celebrity with millions of followers, they may cause a lot of activity when they do something. Hash key doesn't help here, because all writes would go to same key. Solution might be append a 2 digit random number to a key that is known hot so the writes for that key are distributed to the different partitions artificially. 
However, when we use this technique, we have to read from all 100 keys and combine those reads. It doesn't make sense to add this number to all of the keys. 

### Partitioning and Secondary Indexes

The partitioning schemas discussed so far are suitable for key-value indexes like the primary key. The situation becomes more complicated if secondary indexes are involved. A secondary index doesn't identify a record uniquely, but it used for searching occurrences of a particular value. The problem with secondary indexes is that they don't map neatly partitions. There are two main approaches to partition a database with secondary indexes : document-based partitioning and term-based partitioning.

- Document-based partitioning - in this approach, each partition is completely separate, each partition maintains its own secondary indexes, covering only documents in that partition. For that reason document-based partition is known as *local index*. If you want to do a search, you need to send query to all partitions and combine the results you get back. This approach of querying partitions is know as *scatter/gather*. Even if you do all of the queries in parallel, this approach is prone to tail latency amplification.
- Term-based partitioning - Also known as *global index*. The index itself cannot be stored on one node, therefore it is partitioned. (DDIA 208 page has really good graphic). We call this kind of index *term-partitioned*. The advantage of term-partitioned index is that it can make read queries more efficient. Rather than querying all of the partitions for the specific term, only the partitions have the term, need to be queried. The downside is that writes are now slower and more complex. In order to write into term-partitioned index, we need to do a distributed transaction.

### Rebalancing partitions

Over time, things change in database :
- The query throughput increases, so you want to add more CPU to handle the load;
- The dataset size increases, so you want to add more disks and RAM to store it;
- A machine fails and other machines need to take over the failed machine's responsibility.

The process of moving load from one node to another is called *rebalancing*. No matter which partitioning key is used, rebalancing is usually expected to meet some minimum requirements :
- After rebalancing, the load should be shared fairly between the nodes in the cluster.
- While rebalancing is happening, the database should continue accept reads and writes.
- No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network disk I/O load.

Some rebalancing strategies :
- Hash mod n . **BAD IDEA!** If we have nodes numbered from 0-9, you may think that you can just use mod (*%*) operator in order to assign range of hashes to the node, but it's not a good idea, since if the node count changes, most of the keys will need to be moved to meet this strategy and it will get excessively expensive.
- Fixed number of partitions. Create much more partitions that there are nodes. e.g. 10 nodes, 100 partitions = 10 partitions per node. If new nodes are added, they can steal some of the partitions from the current nodes. Number of partitions is usually fixed when data is created and not changed afterwards. Fixed number of partitions is operationally simpler.
- Dynamic partitioning - fixed number of partitions for datasets that rely in range queries, since if you don't get the partition number right, you might end up with a single partitioning that receives all the data. With dynamic partitioning, when a partition grows to a certain size, it is split into two partitions. Also, if a lot of data is deleted, two partitions can be merged into one.  The downside is that if you start with an empty database, all of the writes are processed by one node at the start. This is solved by some databases allowing a certain number of initial partitions. Dynamic partitioning is suitable for key-range and hash partitioning.
- Partitioning proportionally per node - some databases allow you to assign a certain number of partitions per node. This works only with hash-based partitioning.

When doing rebalancing, it's good to have a human in-between the process. Automatic rebalancing can happen in an unintended circumstances like network failure and it is an expensive process. With human oversight, there's less chance for an operation surprise.

### Request routing

When a client makes a request, how does it know which node to connect to? As rebalancing happens, someone needs to stay on top of those changes in order to answer a question, if I want to read "foo", which IP address do I need to connect to?

This is a more general problem called *service discovery*, which is limited only to databases. There are a few different approaches :
1. Allow clients to connect to any node. If that node coincidentally owns the partition to which the request applies, it can handle the request directly. Otherwise it forwards the request to an appropriate node, receives the reply and passes that reply to the client.
2. Send all requests from clients to routing tier first, which determines the node which should handle the request and forwards it accordingly. This routing-tier does not itself handle any requests; it only acts as a partition-aware load balancer.
3. Require the clients be aware of partitioning and assignment of partitions to nodes. In this case, client can connect directly to an appropriate node.

Many distributed data systems rely on a separate coordinator service such as Apache ZooKeeper to keep track of this cluster metadata.