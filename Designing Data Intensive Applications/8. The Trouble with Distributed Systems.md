Anything that can go wrong, will go wrong. There is no reason why software that is running on single computer should be flaky. Same operation should produce same results, we call such system *deterministic*.

The deliberate design choice of computers : if an internal fault occurs, we prefer a computer to crash completely, rather than retuning wrong result, because wrong results are confusing to deal with.

In a distributed system, there may well be some parts of the system that are broken is some unpredictable way, even though other parts of the system are working fine. This is known as *partial failure*.

There are two main ways of building large-scale computing systems :
- **High-performance supercomputing**. Super-computers that have thousands of CPUs are typically used for computationally intensive scientific tasks, such as weather forecasting or molecular dynamics.
- **Cloud-computing**. Often associated with multi-tenant datacenters, connecting with an IP network, elastic/on-demand resource allocation, and metered billing.

Traditional enterprise datacenter lie somewhere between these two extremes.

If we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. We need to build a reliable system from unreliable components.

A system is as reliable is it's least reliable component (*weakest link*).

### Unreliable networks

The internet and most internal networks in datacenters are *asynchronous packet networks*. In this kind of network, one node can send a message to another node, but the network gives no guarantees as to when it will arrive, or whether it will arrive at all. If you send a request and expect response many things could go wrong :

1. Your request may have been lost. (perhaps someone unplugged a network cable)
2. Your request may be waiting in a queue and will be delivered later. (perhaps network or the recipient is overloaded.)
3. The remote node may have failed (perhaps it crashed or it was powered down)
4. The remote node may have temporarily stopped responding, but it will start responding later.
5. The remote node may have processed your request, but the response has been lost on the network.
6. The remote node may have processed your request, but the response has been delayed and will be delivered later.

If you send a request an don't receive a response it is impossible to tell why. The usual way of handling this issue is a *timeout*, after some time you give up waiting and assume that the response won't arrive.

Even if network faults are rare in your environment, the fact that faults *can* occur means that your software needs to be able to handle them. Whenever any communication happens over the network, it may fail - there is way around it. It makes sense to deliberately trigger network problems and test the system's response.

Many systems need to automatically detect faulty nodes :

- A load balancer needs to stop sending requests to a node that is dead.
- In distributed database with single-leader replication, if the leader fails, one of the followers needs to be promoted to be the new leader.

Unfortunately, the uncertainty about the network makes it difficult to tell whether a node is working or not. Even if get an acknowledgement from TCP that a packet was delivered, there's no guarantee that it was processed. If you want to be sure, you need a positive response from the application itself. If something has gone wrong, you may get an error at some level of the stack, but in general you have to assume that you will get no response at all. You can retry a few times, wait for response timeout to elapse and eventually declare the node dead if don't hear back within the timeout.

How long the timeout should be? If timeout is too short, we can prematurely declare node as dead, even if there's a temporary delay in the network.. A long timeout means a long wait until node is declared dead.

TCP considers packet lost if its not acknowledged within some timeout (which is calculated) from observed round-trip times) and lost packets are automatically retransmitted.

UDP does not perform flow control or retransmit lost packets. UDP is a good choice where delayed data is worthless, e.g. conference calls.

One solution would be to make network synchronous. E.g. when a phone call is made over traditional fixed-line network, a circuit in the network is established and space for the call is allocated. This kind of network is called *synchronous*, because space is allocated for the duration of the call. Because there is no queueing, the maximum end-to-end latency is fixed. We call this a *bounded delay.* (I didn't want to go too deep into network hardware, DDIA goes over it in page 283.). 

The internet protocol is designed to be *asynchronous*, because it is designed for *bursty traffic*. In synchronous network, constant amount of data is transmitted where as in the internet that's not the case. Making internet protocol to use synchronous circuit network would waste a lot of resources and make it slow. TCP dynamically adapts the rate of data transfer to the available network capacity. Internet protocol tries to make sure that physical write is utilized to its maximum at all times

### Unreliable Clocks

In distributed network each machine has its own clock, which is an actual hardware device, usually a quartz crystal oscillator. These devices are not perfectly accurate, therefore each machine has its own notion of time.

It's possible to synchronize clocks to some degree. Most common mechanism is Network Time Protocol (NTP), which allows computer clock to be adjust according to the time reported by a group of servers. These servers in turn get their time from a more accurate source.

Modern computers have two different kinds of clocks :

- **Time-of-the day clock** A time of the day clock returns the current date and time according to some calendar. E.g. `clock_gettime(CLOCK_REALTIME)` returns the number of seconds since the *epoch*. Midnight UTC of January 1st, 1970 according to the Gregorian calendar, not counting leap seconds. 
  Time of the day clocks are usually synchronized with NTP. But time of the day clocks are usually prone to some weirdness. If the clock is ahead of NTP server, it can be reset and it would appear that time went backwards. These and other things make time of the day clocks unsuitable for measuring elapsed time.
- **Monotonic clock** `clock_gettime(CLOCK_MONOTONIC)`. They are always guaranteed to move forward. You can check the value of monotonic clock at one point in time, do something, then check the clock again at a later time. *Difference* between the two values tells you how much time elapsed between two checks. However, the *absolute* value of the clock is meaningless. 
  NTP may adjust the frequency at which the monotonic clock moves forward (this is known as *slewing* the clock) if it detects that computer's local quartz is moving faster or slower than the NTP sever. In distributed system, using a monotonic clock for measuring elapsed time is usually fine.

Our methods for getting a clock to tell the correct time aren't nearly as reliable for accurate as one might hope. Google assumes clock drift 200 ppm (parts per million) -> 6ms drift for a clock that is synchronized with a server every 30 seconds. (DDIA page 290 for more issues regarding NTP and clock synchronization, *in general, time of the day clocks report approximate time and may go backwards in some cases.*)

However, it is possible to achieve good accuracy if you are willing to invest significant resources into it. MiFID II draft European regulation for financial institution requires all high-frequency trading funds to synchronize their clocks within 100 microseconds to UTC, in order to help debug market anomalies and detect market manipulation. Such accuracy can be achieved using GPS receivers, the Precision Time Protocol, careful deployment and monitoring. This, however, requires significant effort and expertise.

In leaderless and multi-leader databases timestamps are sometimes used in order to resolve conflicts. This strategy is called last-write-wins. Problems with this approach are :
- Database writes can mysteriously disappear : a node with a lagging clock is unable to overwrite values previously written by a node with a fast clock until the clock skew between nodes has elapsed. This scenario can cause arbitrary amounts of data to be silently dropped without any error being reported to the application.
- LWW cannot distinguish between writes that occurred sequentially in quick succession and writes that were truly concurrent.
- It is possible that two nodes independently generate writes with the same timestamp. An additional tiebreaker value is required to resolve such conflicts.

The solution to this problem is to use *logical clocks*.

It's best to think about clocks as approximate/range values. Google's TrueTime API in Spanner is interesting because it explicitly reports confidence interval of the local clock, you get 2 values `[earliest, latest]`, which are *earliest possible* and *latest possible* timestamps.

When thinking about distributed transactions, which need to reflect causality, monotonic logical clocks are difficult to implement, because of network delay or the fact that each node doesn't necessary known about other nodes logical clock. In Google's Spanner time of the day clocks are used in order to generate transaction ids, however they use range values of TrueTime APi in order to determine whether transactions overlap. If Aearliest < Alatest < Bearliest < Blatest, then transactions for sure do not overlap. In order to keep clock uncertainty as small as possible, Google deploys a GPS receiver or an atomic clock to each of its datacenters.

We have to be especially careful when assuming time difference in distributed systems because a process may stop because of garbage collection, `SIGSTOP`, virtualized environments can suspend process, OS might context switch, etc. We must write distributed software that assumes that it's execution can be paused for an arbitrary amount of time and then resumed later.

### Knowledge, Truth and Lies

A single node cannot rely on it's own judgement and distributed system cannot exclusively rely on a single node, because node may fail at any time, potentially leaving the system stuck and unable to recover. Therefore many system rely on a *quorum*, that is, voting among the nodes. Decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node.

If a quorum decides that a node is dead, it must step down even though it may not be dead.

Most commonly quorum is the absolute majority of more than half nodes.

When using lock or lease to protect access to some resource, such as the file storage, we need to ensure that a node that is under a false belief of being the leader cannot disrupt the rest of the system. A fairly simple technique that achieves this goal is called *fencing*. Every time the lock server grants a lock or lease, it also returns a *fencing token*, which is a number that increases every time a lock is granted. We can require that every time a client makes a call to the storage server, it includes the current fencing token.

*Byzantine fault-tolerant* system is a system that continues to function and operate correctly, even if nodes are malfunctioning and not obeying the protocol, or if malicious attackers are interfering with the network. Byzantine fault-tolerant systems require a supermajority or two-thirds of the nodes to be functioning correctly.

### Summary 

DDIA page 310.