When you are doing a rolling release, it means that old and new changes that were introduced to the system might coexist with one another at one point in time. In order for system to run smoothly, we need to introduce compatibility in both directions.

1. Backward compatibility - newer code can read data that was written by older code.
2. Forward compatibility - older code can read data that was written by newer code.

Backward compatibility is usually pretty easy to achieve, because code author knows what data was written by older code. Forward compatibility is not so easy, because it requires older code to ignore changes that were written by newer code.

### Formats and encoding data

Programs usually work with 2 different representations of data :

1. In memory data that's kept in objects, structs, lists, arrays, hash tables, etc. These data structures are optimised for efficient access and manipulation by CPU.
2. When you write data to a file or send it over the network, you can encode it as some some kind of self-contained sequence of bytes. Since pointers wouldn't make sense in the context of another process, encoded structures look quite different than in-memory ones.

The process of translation from in-memory structure to byte sequence is called *encoding (serialisation, marshalling)* . The reverse is called *decoding (parsing, deserialisation, unmarshalling)*.

It's usually best to avoid **language-specific** encoding formats e.g. `java.io.Serializeable`, `Marshal`, `pickle`.  They introduce many problems e.g. security, versioning, performance, and force you to commit to one language.

Issues of JSON, XML, CSV : 
- There's a lot of ambiguity around encoding numbers. In XML in CSV, numbers are just strings. In JSON you can distinguish between number and a string, however there's no telling whether number is an integer or a float, there's no inherit way to describe precision. This is especially true when dealing with floating-point numbers greater than `2^53`. 
- JSON and XML don't support binary strings. We can encode binary as `base64`, but that increases data size by roughly 33%.
- There's no inherit schema, so correct interpretation of data is up to the application.

JSON, XML and CSV are pretty good for general use-case, but binary encoding formats for, for example, JSON provide better support for schema and offers a more compact structure. (DDIA page 117 has really good visual.)
#### Binary encoding

Binary encoding mechanism such as protocol buffer or Thrift are pretty good at handling backward and forward-compatibility :
- If a new field is added to the schema and old code doesn't recognise it, it can just skip that field. (forward)
- As long as each field has unique tag number, new code can always read new data. The only thing you cannot do is make a field required, because data written by old code, will not have that data.
- You can only remove and optional field. Old code will try to read data written by new code and will throw an exception. (backwards)
- You can never use same tag again. Data written by old code might be incompatible with data used by new code. (forward)

Changing data type of a binary encoded data runs into a risk, that data might get truncated. E.g. changing `int32` into `int64` data type, will get 32 bits truncated when old code reads data.

Binary encoding frameworks usually have a code generation tools which lets the developer to convert the framework schema into a code that efficiently maps encoded data to in-memory structures.

#### Apache Avro

Apache Avro is an interesting framework, as it doesn't include any information about the encoded data type and offers the best compression. However, reader and writer each have their own schema, according to which they encode data type. The question becomes, how does reader know about writer's schema?

There are 3 main use cases for Avro and they each might have different schema resolution type : 
1. Large files with lots of records. We use Avro to store large files, we can include schema at the start of each file. This won't take too much space, considering that the file itself is large.
2. Database with individually written records. Since different records might be written at different points in time with different schemas. The solution is to include schema version in the data. Reader can then identify the version and decode data using that version.
3. Sending records over the network. Sender and receiver must negotiate schema version according to which data will be sent.

### Modes of Dataflow

*Compatibility* - a relationship between one process that encodes that data and another process that decodes it.

Data can flow in 3 distinct ways :
1. Via databases;
2. Via service calls;
3. Via asynchronous message passing.

#### Dataflow through databases

One case of when forward compatibility is not achieved, is when you decode the record from the database and then re-encode it when saving the updated record. In that case, if we the record was written into a database by a newer version of the code with additional fields, those fields could be lost.

**Data outlives code.**

#### Dataflow through services

When you have processes that need to communicate over a network, there are different ways of arranging that communication. The most common arrangement is to have two roles *clients* and *servers*. The servers expose an API over the network, and the clients connect to the servers to make requests to that API. The API exposed by the server is known as a *service*.

Moreover, a server can itself be a client to another service (for example, a typical web app server acts as a client to a database). The approach is often used to decompose a large application into smaller services by area of functionality, such that one service makes request to another when it requires a certain functionality or data. This way of building applications traditionally was called *service-oriented architecture (SOA)* or recently, *microservice architecture*.

A key design goal or service-oriented architecutre is to make application easier to change and maintain by making services independently deployable and evolvable. **Data encoding between servers and clients must be compatible across versions of the service API.**

When HTTP is used as underlying protocol for talking to the service, it is called a *web service*.

There are two popular approaches to we services : 
1. REST - is not a protocol, but rather design philosophy that builds upon the principles of HTTP. It uses URL for identifying resources and using HTTP features for cache control, authentication and content type negotiation. An API design in REST is called *RESTful*. REST = representational state transfer.
2. SOAP - XML-based protocol for making network API requests. SOAP aims to be independent from HTTP and comes with multitude related standards. (the *web service framework*, known as WS-\*). API in SOAP web service is describes using XML-based language called Web Services Description Language. WDSL enabled code generation, so client can access remote service using local classes and methods. SOAP relies on code generation and using programming languages that are not supported by SOAP vendors, makes integration with SOAP services difficult.

#### Issues with remote procedure calls

RPC model tries to make a request to a remote network service look the same as calling a function or method in your programming language within the same process. The approach may seems convenient at first, but it is fundamentally flawed.
