When you are doing a rolling release, it means that old and new changes that were introduced to the system might coexist with one another at one point in time. In order for system to run smoothly, we need to introduce compatibility in both directions.

1. Backward compatibility - newer code can read data that was written by older code.
2. Forward compatibility - older code can read data that was written by newer code.

Backward compatibility is usually pretty easy to achieve, because code author knows what data was written by older code. Forward compatibility is not so easy, because it requires older code to ignore changes that were written by newer code.

### Formats and encoding data

Programs usually work with 2 different representations of data :

1. In memory data that's kept in objects, structs, lists, arrays, hash tables, etc. These data structures are optimised for efficient access and manipulation by CPU.
2. When you write data to a file or send it over the network, you can encode it as some some kind of self-contained sequence of bytes. Since pointers wouldn't make sense in the context of another process, encoded structures look quite different than in-memory ones.

The process of translation from in-memory structure to byte sequence is called *encoding (serialisation, marshalling)* . The reverse is called *decoding (parsing, deserialisation, unmarshalling)*.

It's usually best to avoid **language-specific** encoding formats e.g. `java.io.Serializeable`, `Marshal`, `pickle`.  They introduce many problems e.g. security, versioning, performance, and force you to commit to one language.

Issues of JSON, XML, CSV : 
- There's a lot of ambiguity around encoding numbers. In XML in CSV, numbers are just strings. In JSON you can distinguish between number and a string, however there's no telling whether number is an integer or a float, there's no inherit way to describe precision. This is especially true when dealing with floating-point numbers greater than `2^53`. 
- JSON and XML don't support binary strings. We can encode binary as `base64`, but that increases data size by roughly 33%.
- There's no inherit schema, so correct interpretation of data is up to the application.

JSON, XML and CSV are pretty good for general use-case, but binary encoding formats for, for example, JSON provide better support for schema and offers a more compact structure. (DDIA page 117 has really good visual.)
#### Binary encoding

Binary encoding mechanism such as protocol buffer or Thrift are pretty good at handling backward and forward-compatibility :
- If a new field is added to the schema and old code doesn't recognise it, it can just skip that field. (forward)
- As long as each field has unique tag number, new code can always read new data. The only thing you cannot do is make a field required, because data written by old code, will not have that data.
- You can only remove and optional field. Old code will try to read data written by new code and will throw an exception. (backwards)
- You can never use same tag again. Data written by old code might be incompatible with data used by new code. (forward)

Changing data type of a binary encoded data runs into a risk, that data might get truncated. E.g. changing `int32` into `int64` data type, will get 32 bits truncated when old code reads data.

Binary encoding frameworks usually have a code generation tools which lets the developer to convert the framework schema into a code that efficiently maps encoded data to in-memory structures.

#### Apache Avro

Apache Avro is an interesting framework, as it doesn't include any information about the encoded data type and offers the best compression. However, reader and writer each have their own schema, according to which they encode data type. The question becomes, how does reader know about writer's schema?

There are 3 main use cases for Avro and they each might have different schema resolution type : 
1. Large files with lots of records. We use Avro to store large files, we can include schema at the start of each file. This won't take too much space, considering that the file itself is large.
2. Database with individually written records. Since different records might be written at different points in time with different schemas. The solution is to include schema version in the data. Reader can then identify the version and decode data using that version.
3. Sending records over the network. Sender and receiver must negotiate schema version according to which data will be sent.

### Modes of Dataflow

*Compatibility* - a relationship between one process that encodes that data and another process that decodes it.

Data can flow in 3 distinct ways :
1. Via databases;
2. Via service calls;
3. Via asynchronous message passing.

#### Dataflow through databases

One case of when forward compatibility is not achieved, is when you decode the record from the database and then re-encode it when saving the updated record. In that case, if we the record was written into a database by a newer version of the code with additional fields, those fields could be lost.

**Data outlives code.**

#### Dataflow through services

When you have processes that need to communicate over a network, there are different ways of arranging that communication. The most common arrangement is to have two roles *clients* and *servers*. The servers expose an API over the network, and the clients connect to the servers to make requests to that API. The API exposed by the server is known as a *service*.

Moreover, a server can itself be a client to another service (for example, a typical web app server acts as a client to a database). The approach is often used to decompose a large application into smaller services by area of functionality, such that one service makes request to another when it requires a certain functionality or data. This way of building applications traditionally was called *service-oriented architecture (SOA)* or recently, *microservice architecture*.

A key design goal or service-oriented architecutre is to make application easier to change and maintain by making services independently deployable and evolvable. **Data encoding between servers and clients must be compatible across versions of the service API.**

When HTTP is used as underlying protocol for talking to the service, it is called a *web service*.

There are two popular approaches to we services : 
1. REST - is not a protocol, but rather design philosophy that builds upon the principles of HTTP. It uses URL for identifying resources and using HTTP features for cache control, authentication and content type negotiation. An API design in REST is called *RESTful*. REST = representational state transfer.
2. SOAP - XML-based protocol for making network API requests. SOAP aims to be independent from HTTP and comes with multitude related standards. (the *web service framework*, known as WS-\*). API in SOAP web service is describes using XML-based language called Web Services Description Language. WDSL enabled code generation, so client can access remote service using local classes and methods. SOAP relies on code generation and using programming languages that are not supported by SOAP vendors, makes integration with SOAP services difficult.

#### Issues with remote procedure calls

RPC model tries to make a request to a remote network service look the same as calling a function or method in your programming language within the same process. The approach may seems convenient at first, but it is fundamentally flawed.

- A local function call is predictable and either succeeds or fails, depending only on the parameters are under your control. During a network request, the response might be lost due to a network failure and they are fairly common.
- A local function either returns a result or throws an exception or never returns (because it goes into infinite loop). A request has another possibility. An RPC call may return without result due to *timeout*. In that case we simply don't know what happened.
- If you retry a failed network request, it could happen that the previous request actually got through, only the response was lost. In that case retrying will cause the action to be performed multiple times, unless you build a mechanism for deduplication (*idempotence*).
- Every time you call a local function, it normally takes about the same time to execute. A network request is much slower than a function call, and its latency is also wildly variable.
- When you call a local function, you can efficiently pass it references (pointers) to objects in local memory. When you make a network request, all those parameters need to be encoded into a sequence of bytes that can be sent over the network.
- The client and service may be implemented in different languages, so RPC framework must translate datatypes from one language to another. This might get ugly, since not all languages support all datatypes. E.g. Javascript and numbers greater than `2^53`.

Even though the RPC protocol has it's problems it's not going away any time soon. We just to consider these issues before we jump on the hype train. The main use case for RPC frameworks is on requests between services owned by the same organization, typically within same datacenter.

##### Dataflow and evolution for RPC

For evolvability, it's important that RPC clients and servers can be changed independently. Here we need backward compatibility on requests and forward compatibility with responses.

### Message-passing Dataflow

Using a message broker has several advantages compared to direct RPC :
- It can act as a buffer if the recipient is unavailable or overloaded, and thus improve system reliability.
- It can automatically redeliver messages to a process that has crashed, and thus prevent messages from being lost.
- It avoids sender needing to know the IP address and port number of the recipient (which is particularly useful in cloud deployment where virtual machines often come and go).
- It allows one message to be sent to several recipients.
- It logically decouples the sender from the recipient. (the sender just publishes messages and doesn't care who consumes them).

However, a difference to RPC tis that message-passing communication is usually one-way : a sender normally doesn't expect to receive a reply to its messages. It is possible for a process to send a response, but this would usually be done on a separate channel. This communication pattern is *asynchronous* : the sender doesn't wait for the message to be delivered, but simply sends it and then forgets about it.

Message brokers are used as follows : 
1. One process sends a message to a names *queue* or *topic*.
2. The message broker ensures that the message is delivered to one or more *consumers* or *subscribers*. There can be many consumers on the same topic.

Message brokers typically don't enforce any particular data model. As long as messages are forwards- and backwards-compatible, the flexibility is endless.