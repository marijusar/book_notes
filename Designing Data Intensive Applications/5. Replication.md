*Replication* means keeping a copy of the same data on multiple machines that are connected via network. Reasons why you might want to replicate data : 
- To keep it geographically close to your users (and thus reduce access latency);
- To allow the system to continue working even if some of its parts have failed (and thus increase availability)
- To scale out the number of machines that can serve read queries (and thus increase read throughput).

Algorithms for replicating changes between nodes : *single-leader*, *multi-leader*, *leaderless*.

### Leaders and followers

Each node that stores a copy of the database is called a *replica*.With multiple replicas a question inevitably arises : how do we ensure that all the data ends up on all the replicas?

The most common solution for this is called *leader-based replication*. It works as follows :
1. One of the replicas is designated a *leader*. When clients want to write to the database, they must send their requests to the leader, which first writes the new data to its local storage.
2. The other replicas are known as *followers*. Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of *replication log* or *change stream*. Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader.
3. When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader.

An important detail of the replicated system is whether replication happens *synchronously* or *asynchronously*.

Synchronous follower - leader waits until follower confirms that it has received a write until returning success status to the client and making write visible to the other clients

Asynchronous follower - the leader send the message, but doesn't wait for response.

An advantage of synchronous replication is that follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is if follower doesn't respond, the read cannot be processed. The leader must block all writes and wait until synchronous replica is available.

*Semi-synchronous* configuration is when we have one synchronous follower and rest asynchronous. It's useful, since if followers are synchronous, writes can become awfully slow. If synchronous node crashes or starts responding slowly, an asynchronous follower is made synchronous.

Often leader-based replication is made completely asynchronous. Even though it may seem that the durability suffers from this setup, it's widely used in practice. Leader can continue processing writes, even if all of its followers have fallen behind.

Sometimes you might want to setup additional followers. The process of doing that looks something like this : 
1. Take consistent snapshot of the leader's database at some point in time - if possible, without taking a lock on the entire database. Most databases have this feature, as it is also required for backups.
2. Copy the snapshot to the new follower node.
3. The follower connects to the  leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in leader's replication log. That position might have several names, e.g. : *log sequence number* , *binlog coordinates*.
4. When the follower has processed the backlog of data changes since the snapshot, we say it has *caught up*. It can now continue to process data changes from the leader as they happen.


### Handling outages

Usually the follower can recover quite easily. If it can connect to a leader, it knows the last transaction that was written from its log, and the follower can request all of the further transactions from leader.

Handling failure on leader is tricker, one of the followers needs to be promoted to be the new leader, client needs to be reconfigured to send the writes to the new leader. This process is called *failover*. The process is as follows :

1. *Determining that the leader has failed.* There's no fail proof way to tell whether the leader has failed. Most systems use timeouts. Nodes constantly bounce messages between one another and if, say, one node doesn't respond for 30 seconds, it is assumed dead.
2. *Choosing a new leader*. This could be done through an election process (where the leader is chosen by a majority of the remaining replicas) or a new leader could be appointed by a previously elected *controller node* . The best candidate for a leader is a replica with the most recent changes from old leader.
3. *Reconfiguring the system to use the new leader*. Clients now need to send new write requests to the new leader. If the old leader comes back, it might still believe that it is the leader, not realizing that the other replicated have forced it to step down. The system needs to ensure that the old leader becomes a follower.

Some things can that go wrong with failovers :
- If asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If old leader rejoins the cluster, what should happen to those writes? The new leader might have received conflicting writes in the meantime. The most common solution is for the old leader's un-replicated writes to simply be discarded, which might violates client's durability expectations.
- Discarding writes is especially dangerous if other storage systems outside the database need to coordinated with the database contents. For example if we have an outdated follower that is promoted, and the primary keys, that were used in the old leader weren't replicated to the follower and those keys were saved in Redis. That lead to the wrong primary keys assigned in the new database and some private information being disclosed to wrong users.
- In certain scenarios, it could happen that two nodes both believe that they are the leader. The situation is called *split brain* and it is dangerous. If both leaders accept writes and there is no process for resolving conflicts, data is likely to be lost or corrupted. As a safety catch, some systems have a mechanism to shut down one node if two leaders are detected.
- What is the right timeout before the leader is declared dead? A longer timeout, means a longer time to recovery in the case where leader fails. However, if timeout is too short, there will be unnecessary failovers.

### Implementation of replication logs

##### Statement-based replication

In the simplest case, leader logs every write request that it executes and sends that statement log to its followers, and each follower then executes that SQL statement.

Cases where this doesn't work :
- Any statement that calls non-deterministic function like `NOW()` or `RAND()`.
- If statement use auto-incrementing columns or if they depend on the existing data in the database, e.g. `UPDATE .. WHERE <some condition>`. They must be executed in the exact same order in each replica or else they might have different effect. This can be limiting, when there are multiple concurrently executing transactions.
- Statements that side-effects (e.g. triggers, stored procedures, user-defined functions).

Because there are so many edge cases, often other replication methods are preferable.

##### Write-ahead log shipping

A write-ahead log is an append-only sequence of bytes containing all of the writes tot the database. We can use exact same log to build replica on another node, besides writing log to disk, leader can send it over the network to replicas. Replicas then process the log and build necessary data structures - an exact copy of the leader node.

Because log describes changes on very low level, this makes replication closely coupled with the storage engine. This often disallows different versions of storage engine and makes rolling version upgrades quite difficult.

##### Logical (row-based) replication

An alternative is to use different log formats for replication and for storage engine, which allows replication log to be decouple from storage engine internals. This kind of replication log is called a *logical log*. 

Logical log is a sequence of records, describing writes to database table at the granularity of a row :
- For an inserted row, the log contains new values of all columns;
- For a deleted row, the log contains enough information to uniquely identify the row that was deleted;
- For an updated row, the log contains enough information to uniquely identify the row, and the new values of all columns;

Since logical log is decouple from the storage internals, it can more easily kept backward compatible.

##### Trigger-based replication

A trigger lets you register custom application code that is automatically executed when data changes. It is often used when more flexibility of e.g. data transformation is needed.


### Problems with replication lag

Leader-based replication requires all writes to go through a single node, but read-only queries can go to any replica. For workloads that consist of mostly reads and only a small percentage of writes, there is an attractive option : create many followers, and distribute read requests across those followers.

In this *read-scaling* architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this approach works only with asynchronous replication, otherwise one outage would take down the whole systems ability to process write queries.

Unfortunately, if application reads from asynchronous follower, it may see outdated information if the follower has fallen behind. If you run the same query on leader and the follower at the same time, you might get different results, because not all writes have been reflected in the follower. If you stop writing information to the database and wait a while, the followers will eventually catch up and become consistent with the leader. This effect is known as *eventual consistency*.

When the *replication lag* is large, the inconsistencies it introduces are not just theoretical, but a real problem for applications. Further we will discuss three examples of problems that are likely to occur and outline approaches to solving them.

#### Reading your own writes

When data is written, it must go through the leader, but when data is read, it can go through a follower. This is especially appropriate, when data is frequently viewed, but only occasionally written.

With asynchronous replication, if user views the data shortly after making a write, the new data may not yet have reached the replica. To the user it looks like the submitted data was lost. In this situation we need *read-after-write consistency* also known as *read-your-writes consistency*. (DDIA page 163 has pretty good graphic.) This is a guarantee that after reloading the page user will see updates that they have made themselves. We can achieve that in the following ways :

- When reading something that user has modified, read it from the leader, otherwise read it from the follower. This requires some way to know what has been modified before querying, but it can be resolved in a logical way, e.g. user profile can be modified only by user himself, therefore make own user's profile reads from leader, and other users' profile reads from followers.
- If mostly everything can be edited by the user, this approach doesn't work, because it defeats the purpose of replication. Therefore other rules can be used to decide whether to read from leader or the follower. E.g. if last update to profile was made less than a minute ago, read it from the leader, otherwise read it from the follower.
- The client application can remember the last updated timestamp and when querying replica, application needs to make sure that replica reflects updates at least up to that timestamp. It can be a *logical timestamp* or actual system clock. (system clocks can be less reliable.)
- If replicas are distributed across multiple data centers, there's additional complexity. Any requests that need to be served by the leader, need to routed to the data center, which contains the leader.

Additional complexity arises if we want to provide *cross-device read-after-write-consistency* :
- Approaches that require remembering timestamp of the user's last update, become more difficult, because the coed running on one device, doesn't know what updates have happened on the other device. This metadata needs to be centralized.
- If replicas are distributed across different data-centers, there is no guarantee that connections from different devices will be routed to the same datacenter. If it's required that data is to read from the leader, you may first need to route requests from all of user's devices to the same datacenter.

### Monotonic reads

Another anomaly that occur when reading from asynchronous replicas, is that it's possible for a user to see things *moving backward in time*.

This can happen when a user makes several reads to different replicas. E.g. user `x` posts answer to comment `1234` . User `y` loads the page and sees the reply, he then reloads the page and doesn't see the reply anymore, because he read from different replica. (Once again DDIA page 165 has great graphic.)

*Monotonic reads* is a guarantee that this kind of anomaly doesn't happen. It's lesser guarantee that strong consistency, but lesser guarantee than eventual consistency. One way to achieve this is take make sure that same user always reads from the same replica. However, if replica fails, those reads need to be rerouted to another replica.

### Consistent prefix reads

*Consistent prefix reads* is a guarantees that if certain sequence of writes happen in certain order, then anyone reading them, will see them in the exactly the same order.

Imagine an example of conversation between two people :

```
Mr Poon :
- How far into the future can you see, Mrs. Cake?

Mrs. Cake :
- About ten seconds, usually, Mr. Poon.
```

If consistent prefix read guarantee was not met, then someone could hear this conversation  in the following order :

```
Mrs. Cake :
- About ten seconds, usually, Mr. Poon.
 
Mr Poon :
- How far into the future can you see, Mrs. Cake?
```

Consistent prefix read is important when there's a causal relationship between records and it's a problem in sharded databases. A solution is to make that any data that has causal relationship is written to the same partition. There are also algorithms that explicitly keep track of causal dependencies.

### Solution for Replication Lag

When working with eventually consistent systems, it is worth thinking about how the application behaves if the replication lag increases to several minutes. If answer is "no problem", that's great, but if it results in worse experience for users, it's important to design system with stronger guarantee.