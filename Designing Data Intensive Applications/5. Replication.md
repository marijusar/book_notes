*Replication* means keeping a copy of the same data on multiple machines that are connected via network. Reasons why you might want to replicate data : 
- To keep it geographically close to your users (and thus reduce access latency);
- To allow the system to continue working even if some of its parts have failed (and thus increase availability)
- To scale out the number of machines that can serve read queries (and thus increase read throughput).

Algorithms for replicating changes between nodes : *single-leader*, *multi-leader*, *leaderless*.

### Leaders and followers

Each node that stores a copy of the database is called a *replica*.With multiple replicas a question inevitably arises : how do we ensure that all the data ends up on all the replicas?

The most common solution for this is called *leader-based replication*. It works as follows :
1. One of the replicas is designated a *leader*. When clients want to write to the database, they must send their requests to the leader, which first writes the new data to its local storage.
2. The other replicas are known as *followers*. Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of *replication log* or *change stream*. Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader.
3. When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader.

An important detail of the replicated system is whether replication happens *synchronously* or *asynchronously*.

Synchronous follower - leader waits until follower confirms that it has received a write until returning success status to the client and making write visible to the other clients

Asynchronous follower - the leader send the message, but doesn't wait for response.

An advantage of synchronous replication is that follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is if follower doesn't respond, the read cannot be processed. The leader must block all writes and wait until synchronous replica is available.

*Semi-synchronous* configuration is when we have one synchronous follower and rest asynchronous. It's useful, since if followers are synchronous, writes can become awfully slow. If synchronous node crashes or starts responding slowly, an asynchronous follower is made synchronous.

Often leader-based replication is made completely asynchronous. Even though it may seem that the durability suffers from this setup, it's widely used in practice. Leader can continue processing writes, even if all of its followers have fallen behind.

Sometimes you might want to setup additional followers. The process of doing that looks something like this : 
1. Take consistent snapshot of the leader's database at some point in time - if possible, without taking a lock on the entire database. Most databases have this feature, as it is also required for backups.
2. Copy the snapshot to the new follower node.
3. The follower connects to the  leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in leader's replication log. That position might have several names, e.g. : *log sequence number* , *binlog coordinates*.
4. When the follower has processed the backlog of data changes since the snapshot, we say it has *caught up*. It can now continue to process data changes from the leader as they happen.


### Handling outages

Usually the follower can recover quite easily. If it can connect to a leader, it knows the last transaction that was written from its log, and the follower can request all of the further transactions from leader.

Handling failure on leader is tricker, one of the followers needs to be promoted to be the new leader, client needs to be reconfigured to send the writes to the new leader. This process is called *failover*. The process is as follows :

1. *Determining that the leader has failed.* There's no fail proof way to tell whether the leader has failed. Most systems use timeouts. Nodes constantly bounce messages between one another and if, say, one node doesn't respond for 30 seconds, it is assumed dead.
2. *Choosing a new leader*. This could be done through an election process (where the leader is chosen by a majority of the remaining replicas) or a new leader could be appointed by a previously elected *controller node* . The best candidate for a leader is a replica with the most recent changes from old leader.
3. *Reconfiguring the system to use the new leader*. Clients now need to send new write requests to the new leader. If the old leader comes back, it might still believe that it is the leader, not realizing that the other replicated have forced it to step down. The system needs to ensure that the old leader becomes a follower.

Some things can that go wrong with failovers :
- If asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. If old leader rejoins the cluster, what should happen to those writes? The new leader might have received conflicting writes in the meantime. The most common solution is for the old leader's un-replicated writes to simply be discarded, which might violates client's durability expectations.
- Discarding writes is especially dangerous if other storage systems outside the database need to coordinated with the database contents. For example if we have an outdated follower that is promoted, and the primary keys, that were used in the old leader weren't replicated to the follower and those keys were saved in Redis. That lead to the wrong primary keys assigned in the new database and some private information being disclosed to wrong users.
- In certain scenarios, it could happen that two nodes both believe that they are the leader. The situation is called *split brain* and it is dangerous. If both leaders accept writes and there is no process for resolving conflicts, data is likely to be lost or corrupted. As a safety catch, some systems have a mechanism to shut down one node if two leaders are detected.
- What is the right timeout before the leader is declared dead? A longer timeout, means a longer time to recovery in the case where leader fails. However, if timeout is too short, there will be unnecessary failovers.

### Implementation of replication logs

##### Statement-based replication

In the simplest case, leader logs every write request that it executes and sends that statement log to its followers, and each follower then executes that SQL statement.

Cases where this doesn't work :
- Any statement that calls non-deterministic function like `NOW()` or `RAND()`.
- If statement use auto-incrementing columns or if they depend on the existing data in the database, e.g. `UPDATE .. WHERE <some condition>`. They must be executed in the exact same order in each replica or else they might have different effect. This can be limiting, when there are multiple concurrently executing transactions.
- Statements that side-effects (e.g. triggers, stored procedures, user-defined functions).

Because there are so many edge cases, often other replication methods are preferable.

##### Write-ahead log shipping

A write-ahead log is an append-only sequence of bytes containing all of the writes tot the database. We can use exact same log to build replica on another node, besides writing log to disk, leader can send it over the network to replicas. Replicas then process the log and build necessary data structures - an exact copy of the leader node.

Because log describes changes on very low level, this makes replication closely coupled with the storage engine. This often disallows different versions of storage engine and makes rolling version upgrades quite difficult.

##### Logical (row-based) replication

An alternative is to use different log formats for replication and for storage engine, which allows replication log to be decouple from storage engine internals. This kind of replication log is called a *logical log*. 

Logical log is a sequence of records, describing writes to database table at the granularity of a row :
- For an inserted row, the log contains new values of all columns;
- For a deleted row, the log contains enough information to uniquely identify the row that was deleted;
- For an updated row, the log contains enough information to uniquely identify the row, and the new values of all columns;

Since logical log is decouple from the storage internals, it can more easily kept backward compatible.

##### Trigger-based replication

A trigger lets you register custom application code that is automatically executed when data changes. It is often used when more flexibility of e.g. data transformation is needed.


### Problems with replication lag

Leader-based replication requires all writes to go through a single node, but read-only queries can go to any replica. For workloads that consist of mostly reads and only a small percentage of writes, there is an attractive option : create many followers, and distribute read requests across those followers.

In this *read-scaling* architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this approach works only with asynchronous replication, otherwise one outage would take down the whole systems ability to process write queries.

Unfortunately, if application reads from asynchronous follower, it may see outdated information if the follower has fallen behind. If you run the same query on leader and the follower at the same time, you might get different results, because not all writes have been reflected in the follower. If you stop writing information to the database and wait a while, the followers will eventually catch up and become consistent with the leader. This effect is known as *eventual consistency*.

When the *replication lag* is large, the inconsistencies it introduces are not just theoretical, but a real problem for applications. Further we will discuss three examples of problems that are likely to occur and outline approaches to solving them.

#### Reading your own writes

When data is written, it must go through the leader, but when data is read, it can go through a follower. This is especially appropriate, when data is frequently viewed, but only occasionally written.

With asynchronous replication, if user views the data shortly after making a write, the new data may not yet have reached the replica. To the user it looks like the submitted data was lost. In this situation we need *read-after-write consistency* also known as *read-your-writes consistency*. (DDIA page 163 has pretty good graphic.) This is a guarantee that after reloading the page user will see updates that they have made themselves. We can achieve that in the following ways :

- When reading something that user has modified, read it from the leader, otherwise read it from the follower. This requires some way to know what has been modified before querying, but it can be resolved in a logical way, e.g. user profile can be modified only by user himself, therefore make own user's profile reads from leader, and other users' profile reads from followers.
- If mostly everything can be edited by the user, this approach doesn't work, because it defeats the purpose of replication. Therefore other rules can be used to decide whether to read from leader or the follower. E.g. if last update to profile was made less than a minute ago, read it from the leader, otherwise read it from the follower.
- The client application can remember the last updated timestamp and when querying replica, application needs to make sure that replica reflects updates at least up to that timestamp. It can be a *logical timestamp* or actual system clock. (system clocks can be less reliable.)
- If replicas are distributed across multiple data centers, there's additional complexity. Any requests that need to be served by the leader, need to routed to the data center, which contains the leader.

Additional complexity arises if we want to provide *cross-device read-after-write-consistency* :
- Approaches that require remembering timestamp of the user's last update, become more difficult, because the coed running on one device, doesn't know what updates have happened on the other device. This metadata needs to be centralized.
- If replicas are distributed across different data-centers, there is no guarantee that connections from different devices will be routed to the same datacenter. If it's required that data is to read from the leader, you may first need to route requests from all of user's devices to the same datacenter.

### Monotonic reads

Another anomaly that occur when reading from asynchronous replicas, is that it's possible for a user to see things *moving backward in time*.

This can happen when a user makes several reads to different replicas. E.g. user `x` posts answer to comment `1234` . User `y` loads the page and sees the reply, he then reloads the page and doesn't see the reply anymore, because he read from different replica. (Once again DDIA page 165 has great graphic.)

*Monotonic reads* is a guarantee that this kind of anomaly doesn't happen. It's lesser guarantee that strong consistency, but lesser guarantee than eventual consistency. One way to achieve this is take make sure that same user always reads from the same replica. However, if replica fails, those reads need to be rerouted to another replica.

### Consistent prefix reads

*Consistent prefix reads* is a guarantees that if certain sequence of writes happen in certain order, then anyone reading them, will see them in the exactly the same order.

Imagine an example of conversation between two people :

```
Mr Poon :
- How far into the future can you see, Mrs. Cake?

Mrs. Cake :
- About ten seconds, usually, Mr. Poon.
```

If consistent prefix read guarantee was not met, then someone could hear this conversation  in the following order :

```
Mrs. Cake :
- About ten seconds, usually, Mr. Poon.
 
Mr Poon :
- How far into the future can you see, Mrs. Cake?
```

Consistent prefix read is important when there's a causal relationship between records and it's a problem in sharded databases. A solution is to make that any data that has causal relationship is written to the same partition. There are also algorithms that explicitly keep track of causal dependencies.

### Solution for Replication Lag

When working with eventually consistent systems, it is worth thinking about how the application behaves if the replication lag increases to several minutes. If answer is "no problem", that's great, but if it results in worse experience for users, it's important to design system with stronger guarantee.


## Multi-leader replication

With single-leader architecture there's one downside : all writes must go through one node. If you cannot connect to the leader, for example, due to network interruptions, you can't write to the database. A natural extension to this architecture is to add more leaders. This is called *multi-leader* configuration.

#### Use-cases for multi-leader replication

It rarely makes sense to use multi-leader setup within a single datacenter, because the benefits rarely outweigh the added complexity. Use-cases when multi-leader setup makes sense :

- **Multi-datacenter operation**.  With one-leader setup, the leader must be in one of the datacenters and all writes must go through that datacenter. With multi-leader setup, you can have a leader in each datacenter. Within each datacenter, regular leader-follower replication is used. Between datacenters, each leader replicates its changes to the leaders in other datacenters. 
  Although multi-leader setup has advantages, it also has a big downside that data may be concurrently modified in two different datacenters, and those write-conflicts must be resolved.
  Also, there often subtle configuration pitfalls and surprising interactions with other database features, e.g. auto-incrementing keys, triggers and integrity constraints can be problematic. Thus, multi-leader setups are considered dangerous territory and should be avoided if possible.

- **Clients with offline operation**. Another situation where multi-leader setup is appropriate is if operations need to be continued when client is offline. In this case every local database acts as a leader and there's an asynchronous multi-leader replication process with every other database on other devices. Replication lag depends on when device has network access.

- **Collaborative editing**. One users make changes to their local document, changes are applied to their local replica and then asynchronously replicated to the server and other people's copies.
  
  If you want to prevent conflicts, the application must obtain lock on the document. Before other people can write to that document, the editor must first commit their changes. This is equivalent to single-leader replication.

  However if you want to make collaboration quicker, you can introduce multi-leader replication and requires conflict resolution.

#### Handling write conflicts

- The simplest strategy for dealing with conflicts is to avoid them. If the application can ensure that all writes for a particular record goes through the same leader. Since many implementations of multi-leader replication handle conflicts poorly, this is the recommended approach. 

  E.g. in application where user can edit their own data, you can ensure that all requests for that user go through a particular datacenter. Different users may have different "home" datacenters.


- The other strategy is to resolve conflicts in a *convergent way*, which means that all replicas must arrive at the same value when all changes have been replicated. There are various ways to achieve that :
	- Give each write a unique ID. Pick the write with the highest ID as the *winner* and throw away other writes. If timestamp is used, this technique is know *last write wins*. Although this approach is popular, it is prone to data loss.
	- Give each replica a unique ID and let writes that originated from higher numbered replica always take precedence over writes that originated at a lower-numbered replica. This approach is also prone to data loss.
	- Somehow merge values together. e.g. sort them lexicographically and concatenate them together.
	- Record data conflict in an explicit data structure that preserves all information and write application code that resolves the conflict at some later time (perhaps by prompting the user).

As conflict resolution strategies might depend on the application, most replication tools let you write custom application code to handle conflicts. That code may be executed on write or on read.

*On write* - as soon as database system detects a conflict in the log of replicated changes, it calls the conflict handler.

*On read* - when conflict is detected, all conflicting writes are stored. The next time data is read, these multiple version of the data are returned to the application. The application may prompt the user to automatically resolve the conflict and write back to the database.

Conflict resolution usually applies at the level of an individual row or document, not for an entire transaction. Thus, if you have a transaction that atomically makes several writes, each write is still considered separately for the purposes of conflict resolution.

Specific ways to resolve conflicts that are out of scope of the book : *conflict-free replicated  datatypes*, *mergeable persistent data structures*, *operational transformation*.

#### Multi-leader replication topologies

*Replication topology* describes the communication paths along which writes are propagated from one node to another. Possible topologies : *circular*, *star*, *all-to-all*. (DDIA page 175 has good graphic.)

In circular and star topologies, a write may need to pass through several nodes before it reaches all replicas. To prevent infinite replication loops, each node is given a unique identifier, and in the replication log, each write is tagged with the identifiers of all the nodes it has passed through. If node receives data with it's own identifier, change is ignored.

The problem with circular and star topologies is that if one node fails, it can interrupt the replication process between other nodes, causing them to be unable to communicate until the node is fixed.

All-to-all topology has it's flaws too. Some links can be faster than other and some replication messages can "overtake" others. (DDIA 176 has a good, but complicated graphic.) This can result in causal issues. Causal ordering is necessary to resolve such issues.

## Leaderless replication

Some data storage systems take a different approach than leader-based systems allowing replica directly accept writes from clients. Riak, Cassandra and Voldemort are open source datastore with leaderless replication model. This kind of database is known as *Dynamo-style*. 
**P.S. Dynamo not to be confused with DynamoDB, an AWS product.**

#### Writing to the database when a node is down

In leaderless configuration a failover does not exist. The clients sends write to all replicas. If some node was down while writes have happened, it has become *stale* (outdated). An you may get stale values from the node during reads. To solve this issue, client sends read requests not only to one replica, *read requests are sent to several nodes in parallel*. If replicas return different responses, version numbers are used to determine which value is newer.

Two mechanisms are used in order to ensure that all data eventually is copied to all nodes after they were down: 
- *Read repair* - When a client makes a read from several nodes in parallel, it can get any stale responses. If client sees that it has gotten a stale response, it can write a newer version to the stale node. This approach works well for values where there are frequent reads.
- *Anti-entropy process* - Some datastores have a background process that constantly looks for differences in data between replicas and copies any missing data from one replica to another. Anti-entropy process doesn't copy writes in any particular order and the delay might be significant before data is copied.

Datastores that don't implement read repair, offer reduced durability, because some values might be missing from some replicas.

##### Quorums for reading and writing

If there are *n* replicas, every write must be confirmed by *w* nodes to be considered successful, and we must query at least *r* nodes for each read. As long as *r + w > n*, we expect to get an up-to-date value when reading, because at least *r* nodes we're reading from must be up to date. 

Reads and writes that obey these *r* and *w* values are called *quorum* reads and writes. You can think of *r* and *w* as the minimum votes required for the read or write to be valid.

In Dynamo-style databases, the parameters *n*, *w* and *r* are configurable. A common choice is to make *n* odd number and set *w = r = (n + 1) / 2* (rounded up). Example, setup with few writes and many reads might benefit from setting *w = n = r = 1*. This makes reads faster, but has the disadvantage that just one failed node causes all database writes to fail.

The quorum condition *w + r > n* allows the system to tolerate outages as follows :
- If *w < n* we can still process writes if a node is unavailable.
- If *r < n* we can still process reads if node is unavailable.
- With *n = 3, w = 2,  r = 2*, we can tolerate one unavailable node.
- With *n = 5, w = 3, r = 3*, we can tolerate two unavailable nodes.
- Normally, reads and writes are always sent to all *n* replicas in parallel. The parameters *w* and *r* determine how many nodes we wait for. 

If fewer nodes than *r* or *w* are available, writes or reads return an error. We only care about whether the operation in node was successful and don't need to distinguish what kind of error occurred.

##### Limitations of quorum consistency

If you have *n* replicas and you choose *w* and *r* such that *w + r > n* , you can generally expect every read to return the most recent value written for a key. This is because the set of nodes, to which you have written and the set of nodes from which you've read must overlap.

The smaller *r* and *w* values are, the more likely you are to read stale values, because the more likely is that you have not included the node with the most recent value. On the upside this setup allows lower latency and higher availability.

Possible scenarios where even *w + r > n* satisfied, stale values are returned : 
- If sloppy quorum is used, the *w* writes may end up on different nodes than *r* reads, so there's no longer a guaranteed overlap.
- If two writes occur concurrently it's not clear which occurred first. In this case, the only safe option is to merge concurrent writes.
- If write happens at the same time as a read, write may be reflected on only some of the replicas. Then it's undetermined whether the read returns old or new value.
- If write succeeded on some replicas, but failed on others and overall succeeded writes are *< w*. Writes are not rolled back on replicas where it has succeeded. This means that if a write was reported as failed, subsequent reads may or may not return the value that write.
- If node carrying a new value fails and is restored from replica containing old value, the number of replicas storing new value may fall below *w*, breaking the quorum condition.

### Sloppy Quorums and Hinted Handoff

Databases with appropriately configured quorums can tolerate the failure of individual nodes without the need of failover. They can also tolerate individual nodes going slow, because requests don't have to wait for all *n* nodes. 

In a large cluster (with significantly more than *n* nodes) it's likely that the client can cannot to *some* nodes during network interruption, just not to the nodes that it needs to assemble the quorum for a particular value. In that case, database designers face a trade-off :
- Is it better to return errors to all requests for which we cannot reach a quorum of *w* or *r* nodes?
- Or should we accept writes anyway, and write them to some nodes that are reachable, but aren't among *n* node, on which value usually lives?

The latter is known as *sloppy quorum*. Writes and reads still require *w* and *r* successful responses, but those may include nodes that are not among the designated *n* home nodes for value.

Once the network interruption is fixed, any writes that one node temporarily accepted on behalf of another node are sent to appropriate "home" node. This is called *hinted handoff*.

Leaderless replication is suitable for multi-datacenter operation. The number of replicas *n* includes nodes in all datacenters and you can specify how many *n* replicas you want to have in each datacenter. Data is usually sent to all datacenter, but writes waits for acknowledgment only from quorum of nodes within its local datacenter.