In data systems many things can go wrong :
- The database software or hardware may fail at any time (including middle of the operation);
- The application may crash at any time (including halfway through a series of operations);
- Interruptions in network can unexpectedly cut off application from the database or one node from another;
- Several clients may write to the database at the same time, overwriting each other's changes;
- A client may read data that doesn't make sense because it has only partially been updated;
- Race conditions between clients can cause surprising bugs;

For decades *transactions* have been the mechanism for dealing with such issues. A transaction is way for an application to group several reads or writes together into a logical unit. Entire transaction can either succeed (*commit*) or fail (*abort, rollback*). If it fails, application can safely retry and also we don't need to worry about partial failure scenarios. Using transactions means that the database takes care of potential error scenarios and concurrency issues. (*safety guarantees*)

### The Meaning of ACID

The safety guarantees provided by transactions are often described by the well-known acronym *ACID*, which stands for *Atomicity*, *Consistency*, *Isolation*, *Durability*. However, in practice the implementation of ACID in one database does not to an implementation in another database.

- **Atomicity** - in ACID atomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed - for example, a process crashes, a network connection is interrupted, a disk becomes full or some integrity constrain is violated. If writes cannot be completed when they are grouped in an atomic transaction, and the transaction cannot be completed, then transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.
- **Consistency** - in ACID, consistency is that you have certain statements about your data (*invariants*) that must always be true. If a transaction starts with a database that is valid according to the invariants, and any writes during the transaction preserve the validity, then you can be sure that the invariants are always satisfied. Consistency is a property of an application, rather than database and it's applications responsibility to form transactions that do not violate these invariants. Thus the letter *C* doesn't really belong in ACID.
- **Isolation** - in ACID means that concurrently executing transactions are isolated from each other : they cannot step of each other's toes. Classic database textbooks formalize isolation as *serializability*., which means that each transaction can pretend that is is the only transaction running on the entire database. The database ensures that when the transactions have committed, the result is the same as if they had run *serially* (one after another), even though they might have run concurrently. However, in practice, serializable isolation is rarely used, because it carries a performance penalty.
- **Durability** - in ACID is a promise that once the transaction has committed, any data it has written will not be forgotten, even if there's a hardware fault or the database crashes. In replicated database durability means that the data has been successfully copied to some number of nodes. In order to provide durability guarantee, a database must wait until these writes or replications are complete before reporting a transaction as successfully committed. In reality there are no technique that guarantees absolute durability of data. There are only various risk-reduction techniques like writing to disk, replication and backups.


### Single-Object and Multi-Object Operations

If one transaction reads another transaction's uncommitted writes, we call that a *dirty read*.

Single-object writes can get tricky when we write a lot of data during a single write. What happens if database crashes during the operation? Usually database provides atomicity and isolation for such operations. *Compare-and-set* operations allow a write to only happen if the value has not been concurrently changed by someone.

Reasons why sometimes we need writes to several different objects to be coordinated :
- In relational data model one table often has a foreign key reference to a row in another table. Multi-object transactions allow you to unsure that these references remain valid.
- In a document database, fields that are needed to be updated are often the in same document. In this case no transactions are needed. However, when dealing with multiple documents, since document databases encourage denormalized data, we need to make sure to update these documents in one go. In that case transactions are very useful.
- In databases with secondary indexes, the indexes also need to be updated when changing value. These indexes are different objects from transaction's point of view : without transaction isolation, it's possible for a record to appear in one index, but not another, because update to another index hasn't happened yet.

A key feature of a transactions is that they can be aborted and retried safely. If database detects that the transaction can violate guarantee of atomicity, durability or isolation, it would rather abort the transaction.

DDIA talks a little bit about error handling after aborted transactions. It recommends to retrying transactions after transient errors (deadlocks, network issues), author mentions that there has to be at least some deduplication issues in place if for example transaction actually succeeded, but there was a network issue between client and server, which resulted in application not getting acknowledgement from the server.


### Weak isolation levels

Concurrency issues only occur when transactions read data that is concurrently modified by another transaction or when two transactions try to modify the same data at the same time. Databases try to hide concurrency issues from application by providing *transaction isolation* .*Serializable* isolation has performance issues, therefore databases try to provide weaker isolation guarantees that guard against *some* concurrency issues.

#### Read-committed

The most basic level of transaction isolation is *read-committed*. It makes two guarantees :
1. When reading from the database, you will only see data that is committed (no *dirty reads*. )
2. When writing to the database you will only overwrite data that has been committed (no *dirty writes*)

No dirty reads is a useful guarantee because :
- If a transaction needs to update several objects, a dirty read means that another transaction may see some of the updates but no others. Seeing the database in a partially updated state is confusing to users and may cause other transactions to take incorrect decisions.
- If a transaction aborts, any writes it has made need to be rolled back. If database allows dirty reads, that means a transaction may see data that is later rolled back.

In most of the databases no dirty writes guarantee is usually implemented by locking. In order to write a value to a row, a transaction most acquire a lock and only the transaction that has the the lock can update the row. 
Dirty reads guarantee, however, does not scale well with locks. Since in case of a long-writing transaction all of the reading transactions are blocked. Therefore, most implementations save both old and new values. If another transaction wants to read the value, it is just served and old value.

#### Snapshot Isolation and Repeatable Road

Superficially it may seem that repeatable read guarantees everything that a transaction may require, it is still subject to subtle bugs like *read skew*. (DDIA page 237 has really good graphic and it's tricky to explain race conditions). E.g. Imagine Alice has $1,000 savings at a bank, split across two accounts with $500 each. Now a transactions transfers $100 from one of her accounts to the other. If she is unlucky enough to look at her list of account balance in the same moment as that transaction is being processed, she may see one account balance at a time before the incoming payment has arrived, and other after outgoing transfer has been made. For Alice, it now appears as if she has $900, she has lost $100. If Alice would read the balances again, after the transaction, the first account would have $600. This is an example of *non-repeatable read*.

Even though this is a transient problem, in some cases this is not acceptable :
- Backups - taking backup requires making a copy of an entire database. During that time writes will continue coming in and some parts of backup main contain and older version of the data, other parts - an older version. If you need to restore from backup, these inconsistencies will become permanent.
- Analytical queries - sometimes you want to run analytical queries or integrity checks that ensure that everything is in order. These queries are likely to return nonsensical results if they observe parts of the database at different points in time.

*Snapshot isolation* is the most common solution to this problem. The idea is that each transaction reads from a *consistent snapshot* of the database and it's very useful for long-running read-only queries such a backups.

Like read-committed snapshot isolation uses locks to prevent dirty writes. Key principle is that *readers never block writers, and writers never block readers*. The database must potentially keep several different committed version on an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains different version of an object side by side, this technique is known as *multi-version concurrency control*. Database that support snapshot isolation typically use MVCC for their read committed isolation level as well. A typical approach is that read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction. (DDIA page 240 for graphic of MVCC implementation)

By carefully defining visibility rules, databases are able to able to implement consistent snapshot isolation. This works as follows :
1. At the start of the transaction, the database makes a list of all other transactions that are in progress (not committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.
2. Any writes made by aborted transactions are ignored.
3. Any writes made by transactions with later *transaction ID* (which started after the current transaction has started) are ignored, regardless of what those transactions have committed.
4. All other writes are visible to the application's queries.

An object is visible if both if the following conditions are true :
- At the time when the reader's transaction started, the transactions that created the object had already committed.
- The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader's transaction started.

A long running transaction may continue using a snapshot for a long time, continuing to read values that from other transactions point of view have long been overwritten or deleted. By never updating the values in place but instead creating a new version every time a value is changed, the database can provide a consistent snapshot while incurring only a small overhead.

Some databases call *snapshot isolation* as *repeatable read*.

### Preventing lost updates

Read committed and snapshot isolation levels are primarily focused about the guarantees of what a read-only transaction can see in the presence of concurrent writes. This issue occur during *read-modify-write cycle*. When two concurrently writing transaction update a value and second write does not include the first modification.

One solution is atomic operations, e.g.

```sql
UPDATE counters SET value = value + 1 WHERE key = 'foo';
```

Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. This technique is known as *cursor stability*. Another solution is to make atomic operations to be executed on a single-thread. ORMs make it especially easy to write code that performs read-modify-write cycles instead of using atomic operations provided by the database.

If database doesn't provide a sufficient atomic operation, it is possible to add a lock explicitly and then perform certain logic.

```sql
BEGIN TRANSACTION;

SELECT * FROM figures
	WHERE name ='robot' and game_id = 222
	FOR UPDATE;
-- Check whether move is valid, then update the position
-- of the piece returned by previous select
UPDATE figures SET position = 'c4' WHERE id = 1234;

COMMIT;
```

Some databases e.g. PostgreSQL has automatic lost update detection and abort offending transactions.

Sometime databases that don't implement transactions offer compare-and-set atomic operations, for example :

```sql
UPDATE wiki_pages SET content = 'new content'
WHERE id = 1234 and content = 'old_content'
```

You need to check how your database handles such operation. If this operation can read from old snapshot, this operation will overwrite the current change that was concurrently written by other transaction.

Preventing lost updates takes on another dimension. As discussed before when there are multi-leader and leaderless replication setups, conflicts can occur and there are various conflict resolution techniques. Most databases resolve conflicts by LWW strategy which is prone to data loss.

### Write skew and phantoms

*Write skew* is a race-condition when two concurrent writes read some data and update different objects in such a way incorrect database state is reached. An example two doctors booking time off, when at least one doctor must be on-call. If database evaluates `on_call` condition an the same time and proceeds to update `on_call` status, we might end up with a situation that no doctors are on-call. (DDIA page 247) 

It is neither dirty write , nor lost update because transactions modify different objects. 

Preventing write skew is quite complicated  :
- Atomic single-object operations don't help, as multiple objects are involved.
- Automatic detection of lost updates that you find in the implementation of snapshot isolation, don't help either. Automatically preventing write skew requires true serializable isolation.
- Some databases allow you to configure constraints, which are then enforced by database (e.g. uniqueness, foreign key constraints, or restrictions to a particular value). Most databases do not have constraints that involve multiple objects.
- If you can't use serializable isolation, another option is to explicitly lock all the rows that transaction depends on :

```sql
BEGIN TRANSACTION;

SELECT * FROM doctors
WHERE on_call = true
AND shift_id = 1234 FOR  UPDATE;

UPDATE doctors
SET on_call = false
WHERE name = 'Alice'
AND shift_id = 1234;

COMMIT;
```

Write skew generally follows this scenario :
1. Application uses *SELECT* statement, to check a requirement;
2. Depending on the result of the query application decides to proceed or abort;
3. If application decides to go ahead, it makes a write *INSERT* or *UPDATE*.

In case where we check for the *presence* of some rows, e.g. doctors on call, locking rows is can be done with *FOR UPDATE* statement. However, there are cases when want to check for the *absence* of some rows, then lock cannot be accomplished because during the initial SELECT statement there are no rows present to be locked. This type of write skew in called a *phantom*. Snapshot isolation avoids phantoms only in read-only queries.

In case of phantoms there is no row to attach lock to. In order to solve this problem, we can use technique called *conflict materialization*. In an example, where have a meeting room booking application (DDIA page 249), we can create a table with all of the possible meeting room time slots and use that table for lock acquisition. A note is that we won't store any data in that row, but use it only as a mechanism to allow is to provide a lock to our transactions. Generally, it is only a last-resort technique, because letting concurrency control mechanisms to leak into applications data-model is not a good decision. Serializable isolation level is much more preferred.

### Serializability

Serializable isolation is regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at time, *serially*, without any concurrency. Database prevents *all* possible race conditions.

Databases implement serializability in one of three ways :
- Literally executing transactions in serial order;
- Two-phase locking;
- Optimistic concurrency control techniques such as serializable snapshot isolation.


#### Actual serial execution

The most simple solution is to run all transactions serially, without any concurrency, on a single thread. That way we avoid any possible concurrency issues. 
Systems with single-threaded serial transaction processing don't allow interactive multi-statement transaction. All transaction must commit within a single HTTP request. Application must submit all of the transactions code to the database ahead of time. That's where *stored procedures* are useful.

Stored procedures gained a somewhat bad reputation for various reasons :
- Each database has its own language for stored procedures. These languages haven't kept up with development in general-purpose programming languages, so they look ugly and archaic, from today's POV.
- Code running in database is difficult to manage compared to an application server. It's harder to debug, more awkward to keep in version control and deploy, trickier to test and difficult to integrate with a metrics collection system for monitoring.
- Database is often much more performance sensitive than an application server, because a single database instance is often shared by many application servers. A badly written stored procedure in database can cause much more trouble than equivalent badly written code in an application server.

However these problems can be overcome and some databases use general-purpose programming languages. With stored procedures and in-memory data, executing all transactions on a single thread becomes feasible. As they don't need to wait I/O and they avoid overhead of other concurrency control mechanisms, they can achieve good throughput on a single thread.

Also, if you find a way to correctly partition your data, so that one transaction only needs to touch one partitioning, that could be a good way to horizontally scale serial execution mechanism. However, if transaction needs to touch multiple partitions, then the database must coordinate this transaction throughout the whole distributed system, which is vastly slower.

In summary, serial execution is a viable way of achieving serializable isolation if :
- Every transaction is small and fast;
- Active dataset can fit in-memory;
- Write throughput can be handled on a single CPU core;
- Cross-partition transactions are possible, but there's a hard limit to the extent they can be used.

#### Two-Phase Locking (2PL)

For about 30 years 2PL was used for achieving serializability. All transactions can read on object as long as nobody writes to it. But as soon as someone wants to write an object, exclusive access is required :
- If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts or commits. (This ensures that B can't change the object unexpectedly behind A's back)
- If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts, before it can continue.

In 2PL writers not only block other writers, they also block readers and vice-versa. The lock can be acquired in *shared mode* or in *exclusive mode*. The lock is used as follows :

- If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction has an exclusive lock on the object, these transactions must wait.
- IF a transaction wants to write to an object, it must first acquire the lock in exclusive mode. No other transaction may hold the lock at the same time, so if there is any existing lock on the object, the transaction must wait.
- If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same way as getting an exclusive lock directly.
- After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name *two-phase* comes from : first phase is where the locks are acquired, and the second phase is when all the locks are released.

Using 2PL the transaction throughput is significantly worse than under weak isolation levels. For this reason, databases, that use 2PL can have unstable latencies and they can be very slow in high percentiles.

*Phantom* is a phenomenon when one transaction changes the results of another transactions search query. A database with serializable isolation must prevent phantoms. For that we need a *predicate lock*. It work similarly to the shared/exclusive lock (e.g. one row in a table) , it belongs to all objects that match some search condition such as :

```sql 
SELECT  * FROM bookings 
WHERE room_id = 123 AND
end_time > '2018-01-01 12:00' AND
start_time < '2018-01-01 13:00'
```

A predicate lock restricts access as follows :
- If transaction A wants to read objects matching some condition, like in that SELECT query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object, matching those conditions, A must wait until B releases its lock before it is allowed to make its query.
- If transaction A wants to insert, update or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue.

The key idea is that a predicate lock applies even to objects that do not yet exists in the database, but which might get added in the future (phantoms). If two-phase locking includes predicate locks, the database prevents all forms of write skew and other race conditions, and so its isolation becomes serializable.

Predicate locks do not perform well, because checking for locks becomes time consuming , so most databases implement *index-range locking* instead, which is simplified approximation of predicate locking.

If you have predicate lock for room 123 between noon and 1 PM, you can approximate it by locking bookings for room 123 at any time or you can approximate it by locking all room between noon and 1PM. In the bookings database you would probably have an index on the `room_id` column, and/or indexes on `start_time` and `end_time`.

- Say your index is on `room_id`, and the database uses this index to find existing bookings for room 123. Now database can simply attach a shared lock to this index entry, indicating that a transaction has searches for bookings of room 123.
- Alternatively, if database uses a time-based index to find existing bookings, it can attach a shared lock to range of values in that index, indicating that a transaction has searched for booking that overlap with time period of noon to 1PM of January 1, 2018.

Index-range locks are not as precise as predicate locks would be, but since they have a much lower overhead, they are a good compromise. If there's no index to attach to, the database can attach lock a the whole table. It's not good for performance, but it's a good fallback position.


### Serializable snapshot isolation

Serializable snapshot algorithm is a new algorithm, discovered in 2008, that offers full serializability and small performance penalty. SSI is an *optimistic* concurrency control mechanism, meaning that database allows for transactions to run concurrently, in hope that everything will turn out okay. When transaction wants to commit, the database checks whether isolation was violated. If so the transaction is aborted.

If there is high enough space capacity and the contention between transactions is not too high, optimistic concurrency control techniques tend to perform better than pessimistic ones. Contention can be reduced by using atomic operations. SSI uses snapshot isolation and adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.

There are 2 ways to prevent phantoms and write-skew in SSI. Phantoms and write-skew happen when database writes an object based on outdated *premise*. 

- Detecting reads of a stale MVCC object version.  In order to prevent stale MVCC reads, the database needs to track when a transaction ignores another transaction's writes due to MVCC visibility rules. When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted. By avoiding unnecessary aborts, SSI preservers snapshot isolation's support for long-running reads from consistent snapshot.
- Detecting writes that affect prior reads. SSI locks don't block other transactions. When transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. This process is similar to acquired a write lock on the affected key range, but rather than blocking until the readers have committed, the locks acts as a tripwire : it simply notifies the transactions that the data they read may no longer be up to date. 

A great upside of SSI compared to 2PL is that one transaction doesn't need to block waiting for locks held by another transaction. Like under snapshot isolation, writers don't block readers and readers don't block writers. This design principle makes query latency much more predictable and less variable. In particular read-only queries can run on consistent snapshot without requiring any lock, which is appealing for read-heavy workloads. Performance of SSI heavily depends on how long the transactions run. SSI requires read-write transactions to be fairly short, however it is less sensitive than 2PL.



### Summary

*Taken from DDIA 266*.

Transactions are an abstraction layer that allows an application to pretend that certain concurrency problems and certain kinds of hardware and software faults don’t exist. A large class of errors is reduced down to a simple transaction abort, and the application just needs to try again.

In this chapter we saw many examples of problems that transactions help prevent. Not all applications are susceptible to all those problems: an application with very simple access patterns, such as reading and writing only a single record, can probably manage without transactions. However, for more complex access patterns, transactions can hugely reduce the number of potential error cases you need to think about.

Without transactions, various error scenarios (processes crashing, network interruptions, power outages, disk full, unexpected concurrency, etc.) mean that data can become inconsistent in various ways. For example, denormalized data can easily go out of sync with the source data. Without transactions, it becomes very difficult to reason about the effects that complex interacting accesses can have on the database. In this chapter, we went particularly deep into the topic of concurrency control. We discussed several widely used isolation levels, in particular read committed, snapshot isolation (sometimes called repeatable read), and serializable. We characterized those isolation levels by discussing various examples of race conditions:

- **Dirty reads**. One client reads another client’s writes before they have been committed. The read committed isolation level and stronger levels prevent dirty reads.
- **Dirty writes**. One client overwrites data that another client has written, but not yet committed. Almost all transaction implementations prevent dirty writes.
- **Read skew (nonrepeatable reads)**. A client sees different parts of the database at different points in time. This issue is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with multi-version concurrency control (MVCC).
- **Lost updates**. Two clients concurrently perform a read-modify-write cycle. One overwrites the other’s write without incorporating its changes, so data is lost. Some implementations of snapshot isolation prevent this anomaly automatically, while others require a manual lock (SELECT FOR UPDATE).
- **Write skew** A transaction reads something, makes a decision based on the value it saw, and writes the decision to the database. However, by the time the write is made, the premise of the decision is no longer true. Only serializable isolation prevents this anomaly.
- **Phantom reads** A transaction reads objects that match some search condition. Another client makes a write that affects the results of that search. Snapshot isolation prevents straightforward phantom reads, but phantoms in the context of write skew require special treatment, such as index-range locks.


Weak isolation levels protect against some of those anomalies but leave you, the application developer, to handle others manually (e.g., using explicit locking). Only serializable isolation protects against all of these issues. We discussed three different approaches to implementing serializable transactions:
- **Literally executing transactions in a serial order** If you can make each transaction very fast to execute, and the transaction throughput is low enough to process on a single CPU core, this is a simple and effective option.
- **Two-phase locking** For decades this has been the standard way of implementing serializability, but many applications avoid using it because of its performance characteristics.
- **Serializable snapshot isolation (SSI)** A fairly new algorithm that avoids most of the downsides of the previous approaches. It uses an optimistic approach, allowing transactions to proceed without blocking. When a transaction wants to commit, it is checked, and it is aborted if the execution was not serializable.

The examples in this chapter used a relational data model. However, as discussed in “The need for multi-object transactions” on page 231, transactions are a valuable database feature, no matter which data model is used.

In this chapter, we explored ideas and algorithms mostly in the context of a database running on a single machine. Transactions in distributed databases open a new set of difficult challenges, which we’ll discuss in the next two chapters.