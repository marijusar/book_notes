*Consensus* - getting all nodes to agree on something. Reliably reaching consensus in spite of network faults and process failures is a surprisingly tricky problem.

### Linearizability

Linearizability gives us an illusion that there is a single replica in the system and all operations are atomic, when in fact there are multiple replicas. Linearizability is a *recency guarantee*. If one read returns a new value after write, all subsequent reads on replicas should return the new value.

DDIA gives a couple pages of examples of linearizability, starting page 324, but I didn't feel like including them here.

Linearizability is easy to confuse with serializability. 
- Serializability is a property of *transactions*. It guarantees that transactions behave as though they have been executed is serial order. 
- Linearizability is a recency guarantee for read and writes. It doesn't group operations together into transactions and it doesn't prevent problems such as write skew, unless you take measures such as materializing conflicts.


Linearizability is useful for locking services for distributed databases, especially when promoting nodes to leader. We must make sure that every node has the same opinion on who is the leader. Linearizability is also useful when implementing uniqueness guarantees.

Systems that use cross-channel dependencies need to be linearizable. Imagine that you have an application which resizes images. Application uploads an image to a file storage and publishes a message to the message broker, if worker consumes the message before file is replicated across the distributed file system, it can encounter an error while trying to download the image from file storage. In such case system is not linearizable because of additional communication channel.

Replication methods and their linearizability :
- Single-leader replication. Potentially linearizable as long as you read from the leader or synchronous followers.
- Consensus algorithms. Linearizable.
- Multi-leader replication. Not linearizable.
- Leaderless replication. Probably not linearizable. Good example in DDIA 334, it is possible to make leaderless replication linearizable if reader performs read repairs before returning data to the application and writer must read the latest state of a quorum of nodes before sending its writes. However, this comes at performance penalty and LWW conflict resolution strategy cannot be used. It's safest to assume that Dynamo-style replication does not provide linearizability.

Applications that don't require linearizability, can be more fault tolerant to the network faults. If replica is disconnected from leader, then it cannot perform linearizable read requests and must wait until the connection is restored.

CAP theorem - consistency, availability, partition tolerance, pick 2 out of 3. *Either consistent or available when partitioned.* Consistent = linearizable in terms of CAP theorem.

Linearizability is slow. Even RAM is not linearizable, because CPU is reads data from it's own cache and then asynchronously replicates data from main memory.

### Ordering and Causality

One of the reasons why we bring up ordering is that it help to preserve *causality*. We say that there's a *causal dependency* between question and the answer. Causality imposes an ordering on events : cause comes before effect; a message is sent before message is received; a question comes before an answer.

Read skew means the data is in a state that violates causality.

If system obeys the ordering imposed by causality, we say it is *causally consistent*. 

*Total order* allows any two elements to be compared, if you have two elements, you can always say which one is greater and which one is smaller. In  linearizable system, we have a *total order* of operations, we can always say which happened first. However, if operations are concurrent, that means that they have happened at the same time, no operation happened first, therefore we can say that in linearizable system there is no concurrency. There must be a single timeline along which all operations are totally ordered.

Linearizability *implies* causality, any system that is linearizable will preserve causality. 

System can be causally consistent, without being linearizable and taking the performance hit. In fact causal consistency is the strongest consistency model that doesn't suffer from the network delays and remains available if the face of network failure.

In many cases, systems that appear to require linearizability only require causal consistency.

### Sequence Number Ordering

A good way to track causal dependencies is to assign *sequence number or timestamp* to the operations. Timestamps not necessarily need to come from time-of-the-day clock, but it can come from *logical clock*, which can be implemented using counters.

Such sequence numbers are compact and they provide total ordering. This works if there's a single leader, but if there are multiple leaders or no leaders, it gets more tricky. For this case we can use *Lamport timestamp*. It is simply a pair of *(counter, node ID)*

If the counter values are the same, the one with greater node ID is the greater timestamp. Every node and every client keeps track of the *maximum* counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.

### Total Order Broadcast

Total order broadcast is usually described as a protocol for exchanging messages between nodes. It requires 2 safety properties to be satisfied :

- **Reliable delivery**. No messages are lost. If one message is delivered to one node, it is delivered to all nodes.
- **Totally ordered delivery**. Messages are delivered to every node in the same order.

If every message represents a write to the database, and every replica processes the same writes in the same order, then replicas will remain consistent with each other. This principle is known as *state machine replication*.

Total order broadcast does not guarantee linearizability. Total order broadcast only guarantees that messages will be delivered in the same order, but there is no guarantee *when* it will be delivered. By contrast, linearizability is a recency guarantee.


### Distributed transactions and consensus

There are number of situations where nodes have to agree :
- **Leader election**. It is important to avoid bad failover and split brain situation, where two nodes think of themselves as leaders. If there were two leaders, they would both accept writes and their data would diverge, leading to inconsistency and data loss.
- **Atomic commit**. In a database, that supports transactions spanning over several nodes or partitions, we have the problem, that transaction may fail on some nodes but succeed on other. If we want to maintain transaction atomicity, we have to make all nodes agree on then outcome.


Two-phase commit is an algorithm for achieving atomic transaction commit across multiple nodes. 2PC uses a new component that does not normally appear in single-node transactions : *coordinator*. The coordinator is often implemented as a library within the same application process that is requesting the transaction, but it can also be a separate process or service. 
When the transaction is ready commit, the coordinator begins phase 1, it sends a *prepare* request to each of the nodes,  asking them whether they are able to commit. The coordinator track responses from the participants :
- If all participants reply "yes", indicating that they are ready to commit, then coordinator send out a *commit* request in phase 2, and the commit actually takes place.
- If any of the participants replies "no", the coordinator sends *abort* request to all nodes in phase 2.

If commit or abort requests fail due to network or something else, coordinator retries indefinitely. If the coordinator fails, participants can safely abort the transaction. However, if participant voted "yes", it cannot abort transaction unilaterally, it must wait to hear from coordinator, whether transaction was committed or aborted. Such state is called *in doubt*. 

*DDIA has good example on page 357*.

There are two types of distributed transactions :

- **Database-internal distributed transactions** - a transaction over multiple nodes that are sharing the same version of the same software.
- **Heterogeneous distributed transactions** - a setup where there might be different versions of different software running.

XA transactions is a standard for implementing two-phase commit across heterogeneous technologies.

When coordinator restarts, sometimes it can result in *orphaned* transactions. This can resolved in some locks never getting released in the database until it is resolved manually.

*Probably need to reread this chapter, because a lot of ideas are pretty abstract.*